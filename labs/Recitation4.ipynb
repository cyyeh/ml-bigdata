{"cells":[{"cell_type":"markdown","source":["# Learning Rate Schedule \n\n* A CNN model is constructed to train on CIFAR-10\n\n* The following learning rate decay schedules are used : constant learning rate, time-based decay, step decay, exponential decay\n    \n* Model performance of using different learning rate schedules are compared"],"metadata":{}},{"cell_type":"code","source":["from __future__ import print_function\nimport numpy as np\nimport math\n\nimport matplotlib.pyplot as plt\n# If you're running this notebook locally, uncomment this line\n# %matplotlib inline  \n\nimport keras\nfrom keras import backend as K\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.models import model_from_json\nfrom keras.callbacks import LearningRateScheduler"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["# Load CIFAR-10 data"],"metadata":{}},{"cell_type":"code","source":["batch_size = 64\nnum_classes = 2\nepochs = 50\n\n# input image dimensions\nimg_rows, img_cols = 32, 32   \n\n# the data, shuffled and split between train and test sets\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()   \n\n# Only look at cats [=3] and dogs [=5]\ntrain_picks = np.ravel(np.logical_or(y_train==3,y_train==5))  \ntest_picks = np.ravel(np.logical_or(y_test==3,y_test==5))     \n\ny_train = np.array(y_train[train_picks]==5,dtype=int)\ny_test = np.array(y_test[test_picks]==5,dtype=int)\n\nX_train = X_train[train_picks]\nX_test = X_test[test_picks]\n\n# Re-format according to Keras's setting\n# channel: RGB channels\nif K.image_data_format() == 'channels_first':\n    X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n    input_shape = (3, img_rows, img_cols)\nelse:\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n    input_shape = (img_rows, img_cols, 3)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(np.ravel(y_train), num_classes)\ny_test = keras.utils.to_categorical(np.ravel(y_test), num_classes)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n\n     8192/170498071 [..............................] - ETA: 24:39\n    40960/170498071 [..............................] - ETA: 9:54 \n   106496/170498071 [..............................] - ETA: 5:42\n   221184/170498071 [..............................] - ETA: 3:39\n   450560/170498071 [..............................] - ETA: 2:14\n   892928/170498071 [..............................] - ETA: 1:21\n  1794048/170498071 [..............................] - ETA: 46s \n  3596288/170498071 [..............................] - ETA: 26s\n  6725632/170498071 [&gt;.............................] - ETA: 15s\n  9773056/170498071 [&gt;.............................] - ETA: 11s\n 12886016/170498071 [=&gt;............................] - ETA: 9s \n 15638528/170498071 [=&gt;............................] - ETA: 8s\n 18685952/170498071 [==&gt;...........................] - ETA: 7s\n 21708800/170498071 [==&gt;...........................] - ETA: 6s\n 23388160/170498071 [===&gt;..........................] - ETA: 6s\n 25100288/170498071 [===&gt;..........................] - ETA: 6s\n 27516928/170498071 [===&gt;..........................] - ETA: 5s\n 30466048/170498071 [====&gt;.........................] - ETA: 5s\n 33497088/170498071 [====&gt;.........................] - ETA: 5s\n 36315136/170498071 [=====&gt;........................] - ETA: 4s\n 38035456/170498071 [=====&gt;........................] - ETA: 4s\n 39624704/170498071 [=====&gt;........................] - ETA: 4s\n 42098688/170498071 [======&gt;.......................] - ETA: 4s\n 44900352/170498071 [======&gt;.......................] - ETA: 4s\n 47816704/170498071 [=======&gt;......................] - ETA: 4s\n 50929664/170498071 [=======&gt;......................] - ETA: 4s\n 53780480/170498071 [========&gt;.....................] - ETA: 3s\n 56713216/170498071 [========&gt;.....................] - ETA: 3s\n 59654144/170498071 [=========&gt;....................] - ETA: 3s\n 61399040/170498071 [=========&gt;....................] - ETA: 3s\n 63135744/170498071 [==========&gt;...................] - ETA: 3s\n 65347584/170498071 [==========&gt;...................] - ETA: 3s\n 68116480/170498071 [==========&gt;...................] - ETA: 3s\n 70721536/170498071 [===========&gt;..................] - ETA: 3s\n 73719808/170498071 [===========&gt;..................] - ETA: 3s\n 76554240/170498071 [============&gt;.................] - ETA: 2s\n 79650816/170498071 [=============&gt;................] - ETA: 2s\n 82714624/170498071 [=============&gt;................] - ETA: 2s\n 85368832/170498071 [==============&gt;...............] - ETA: 2s\n 87990272/170498071 [==============&gt;...............] - ETA: 2s\n 91086848/170498071 [===============&gt;..............] - ETA: 2s\n 94134272/170498071 [===============&gt;..............] - ETA: 2s\n 97001472/170498071 [================&gt;.............] - ETA: 2s\n 99983360/170498071 [================&gt;.............] - ETA: 2s\n102981632/170498071 [=================&gt;............] - ETA: 1s\n105717760/170498071 [=================&gt;............] - ETA: 1s\n108486656/170498071 [==================&gt;...........] - ETA: 1s\n111386624/170498071 [==================&gt;...........] - ETA: 1s\n113475584/170498071 [==================&gt;...........] - ETA: 1s\n115720192/170498071 [===================&gt;..........] - ETA: 1s\n116629504/170498071 [===================&gt;..........] - ETA: 1s\n119463936/170498071 [====================&gt;.........] - ETA: 1s\n121856000/170498071 [====================&gt;.........] - ETA: 1s\n123895808/170498071 [====================&gt;.........] - ETA: 1s\n125140992/170498071 [=====================&gt;........] - ETA: 1s\n127623168/170498071 [=====================&gt;........] - ETA: 1s\n130752512/170498071 [======================&gt;.......] - ETA: 1s\n133685248/170498071 [======================&gt;.......] - ETA: 1s\n136585216/170498071 [=======================&gt;......] - ETA: 0s\n139386880/170498071 [=======================&gt;......] - ETA: 0s\n142434304/170498071 [========================&gt;.....] - ETA: 0s\n144818176/170498071 [========================&gt;.....] - ETA: 0s\n146694144/170498071 [========================&gt;.....] - ETA: 0s\n148340736/170498071 [=========================&gt;....] - ETA: 0s\n150593536/170498071 [=========================&gt;....] - ETA: 0s\n153706496/170498071 [==========================&gt;...] - ETA: 0s\n156426240/170498071 [==========================&gt;...] - ETA: 0s\n159244288/170498071 [===========================&gt;..] - ETA: 0s\n162029568/170498071 [===========================&gt;..] - ETA: 0s\n163602432/170498071 [===========================&gt;..] - ETA: 0s\n165199872/170498071 [============================&gt;.] - ETA: 0s\n167796736/170498071 [============================&gt;.] - ETA: 0s\n170500096/170498071 [==============================] - 5s 0us/step\nX_train shape: (10000, 32, 32, 3)\n10000 train samples\n2000 test samples\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["# Define function to construct CNN model"],"metadata":{}},{"cell_type":"code","source":["def cnn_model() : \n    model = Sequential()\n    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n    model.add(Conv2D(8, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(16, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(2, activation='softmax'))\n    return(model)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["# Define function to plot model accuracy"],"metadata":{}},{"cell_type":"code","source":["def plot_fig(i, history):\n    fig = plt.figure()\n    plt.plot(range(1,epochs+1),history.history['val_accuracy'],label='validation')\n    plt.plot(range(1,epochs+1),history.history['accuracy'],label='training')\n    plt.legend(loc=0)\n    plt.xlabel('epochs')\n    plt.ylabel('accuracy')\n    plt.xlim([1,epochs])\n    plt.grid(True)\n    plt.title(\"Model Accuracy\")\n    # If you're running this notebook on databricks, use display to generate graphs\n    display(fig)\n    # If you're running this notebook locally, use plt.show to generate graphs\n    # plt.show()\n    # plt.close(fig)\n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["# Constant learning rate"],"metadata":{}},{"cell_type":"code","source":["# define CNN model\nmodel1 = cnn_model()\n\n# define SGD optimizer\nlearning_rate = 0.1\nsgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False) \n\n# compile the model\nmodel1.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=['accuracy'])\n\n# fit the model\nhistory1 = model1.fit(X_train, y_train,\n                batch_size=batch_size,\n                epochs=epochs,\n                verbose=2,\n                validation_data=(X_test, y_test))\n\n# plot model accuracy\nplot_fig(1, history1)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Time-based decay"],"metadata":{}},{"cell_type":"code","source":["learning_rate = 0.1\ndecay_rate = learning_rate / epochs\nlrs = [learning_rate] * epochs\nfor i in range(1, epochs):\n    lrs[i] = lrs[i - 1] * (1. / (1. + decay_rate * i))\n\n# plot learning rate\nfig = plt.figure()\nplt.plot(range(1,epochs+1),lrs,label='learning rate')\nplt.xlabel(\"epoch\")\nplt.xlim([1,epochs+1])\nplt.ylabel(\"learning rate\")\nplt.legend(loc=0)\nplt.grid(True)\nplt.title(\"Learning rate\")\n# If you're running this notebook on databricks, use display to generate graphs\ndisplay(fig)\n# If you're running this notebook locally, use plt.show to generate graphs\n# plt.show()\n# plt.close(fig)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# define CNN model\nmodel2 = cnn_model()\n\n# define SGD optimizer\nlearning_rate = 0.1\ndecay_rate = 0.002 # learning_rate / epochs\nmomentum = 0.5\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n\n# compile the model\nmodel2.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=['accuracy'])\n\n# fit the model\nhistory2 = model2.fit(X_train, y_train, \n                     epochs=epochs, \n                     batch_size=batch_size,\n                     verbose=2, \n                     validation_data=(X_test, y_test))\n\n# plot model accuracy\nplot_fig(2, history2)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# Step decay"],"metadata":{}},{"cell_type":"code","source":["learning_rate = 0.1\ndrop = 0.5\nepochs_drop = 10.0\nlrs = [learning_rate] * epochs\nfor i in range(1, epochs):\n    lrs[i] = learning_rate * math.pow(drop, math.floor((i)/epochs_drop))\n  \n# plot learning rate\nfig = plt.figure()\nplt.plot(range(1,epochs+1),lrs,label='learning rate')\nplt.xlabel(\"epoch\")\nplt.xlim([1,epochs+1])\nplt.ylabel(\"learning rate\")\nplt.legend(loc=0)\nplt.grid(True)\nplt.title(\"Learning rate\")\n# If you're running this notebook on databricks, use display to generate graphs\ndisplay(fig)\n# If you're running this notebook locally, use plt.show to generate graphs\n# plt.show()\n# plt.close(fig)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# define CNN model\nmodel3 = cnn_model()\n\n# define SGD optimizer\nmomentum = 0.5\nsgd = SGD(lr=0.0, momentum=momentum, decay=0.0, nesterov=False) \n\n# compile the model\nmodel3.compile(loss=keras.losses.categorical_crossentropy,optimizer=sgd, metrics=['accuracy'])\n\n# define step decay function\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(drop, math.floor((epoch)/epochs_drop))\n    return lrate\n\n# learning schedule callback\nlrate = LearningRateScheduler(step_decay)\ncallbacks_list = [lrate]\n\n# fit the model\nhistory3 = model3.fit(X_train, y_train, \n                     validation_data=(X_test, y_test), \n                     epochs=epochs, \n                     batch_size=batch_size, \n                     callbacks=callbacks_list, \n                     verbose=2)\n\n# plot model accuracy\nplot_fig(3, history3)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["# Exponential decay"],"metadata":{}},{"cell_type":"code","source":["learning_rate = 0.1\nk = 0.1\nlrs = [learning_rate] * epochs\nfor i in range(1, epochs):\n    lrs[i] = learning_rate * np.exp(-k*i)\n\n# plot learning rate\nfig = plt.figure()\nplt.plot(range(1,epochs+1),lrs,label='learning rate')\nplt.xlabel(\"epoch\")\nplt.xlim([1,epochs+1])\nplt.ylabel(\"learning rate\")\nplt.legend(loc=0)\nplt.grid(True)\nplt.title(\"Learning rate\")\n# If you're running this notebook on databricks, use display to generate graphs\ndisplay(fig)\n# If you're running this notebook locally, use plt.show to generate graphs\n# plt.show()\n# plt.close(fig)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# define CNN model\nmodel4 = cnn_model()\n\n# define SGD optimizer\nmomentum = 0.5\nsgd = SGD(lr=0.0, momentum=momentum, decay=0.0, nesterov=False)\n\n# compile the model\nmodel4.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=['accuracy'])\n\n# define step decay function\ndef exp_decay(epoch):\n    initial_lrate = 0.1\n    k = 0.1\n    lrate = initial_lrate * np.exp(-k*epoch)\n    return lrate\n\n# learning schedule callback\nlrate_ = LearningRateScheduler(exp_decay)\ncallbacks_list_ = [lrate_]\n\n# fit the model\nhistory4 = model4.fit(X_train, y_train, \n     validation_data=(X_test, y_test), \n     epochs=epochs, \n     batch_size=batch_size, \n     callbacks=callbacks_list_, \n     verbose=2)\n\n# plot model accuracy\nplot_fig(4, history4)"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"recitation","notebookId":3747849945748512},"nbformat":4,"nbformat_minor":0}
