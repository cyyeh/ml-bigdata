# ML with Large Datasets: Intro to Spark

###### tags: `ml`

## Recall

- what are two qualities of Spark's RDDs that give us fault tolerance while being able to do computations in-memory?
    - immutability
    - functional transformations
- architecture of a Spark job
    ![](https://i.imgur.com/D5PyLmp.png)

## Outline

![](https://i.imgur.com/DHDakov.png)

## Lecture Summaries

- operations on RDDs
    - transformations
        - function: return new RDDs as a result
        - semantics: computed lazily
        - Spark leverages this by analyzing and optimizing the chain of operations before executing it
        ![](https://i.imgur.com/dpKb2lR.png)
    - actions
        - function:compute a result based on an RDD, and either returned or saved to an external storage system(e.g., HDFS)
        - semantics: computed eagerly
=        ![](https://i.imgur.com/Yey8wHw.png)
    - **laziness/eagerness is how we can limit network communication using the programming model**
    - using Python lambda functions
        - keep in mind that you are restricted to a single expression
- how do we create an RDD?
    - transforming an existing RDD
    - from a SparkContext(or SparkSession)
        - it represents the connection between the Spark cluster and your running application.
            - `parallelize`: convert a local Python list to an RDD
            - `textFile`: read a textfile from HDFS or a local file sytem and return an RDD containing strings
- caching and persistence
    - by default, RDDs are recomputed each time you run an action on them
    - you can cache RDDs in memory, simply call `persist()` or `cache()` on it
    - possible ways to persist data
    ![](https://i.imgur.com/Ty2diQk.png)
    - `cache()`
        - shorthand for using the default storage level, which is in memory only as regular Java objects
    - `persist()`
        - persistence can be customized with this method. Pass the storage level you'd like as a parameter to persist
        ![](https://i.imgur.com/4qXidqr.png)
    - **one of the most common performance bottlenecks of newcomers to Spark arises from unknowingly re-evaluating several transformations when caching could be used**

