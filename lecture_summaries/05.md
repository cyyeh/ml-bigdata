# ML with Large Datasets: Spark: Joins, Structure, and DataFrames

###### tags: `ml`

## Lecture Summaries

- all data isn't equal, structurally. It falls on a spectrum from unstructured to structured
    ![](https://i.imgur.com/ha9cGcC.png)
- structured vs unstructured
    - unstructured: (Spark RDDs)
        ![](https://i.imgur.com/hKbUK91.png)
        - not much structure
        - difficult to aggresively optimize
    - structured: (Databases/Hive)
        ![](https://i.imgur.com/7Ls59FY.png)
        - lots of structure
        - lotf of optimization opportunities
        - DataFrames & Spark SQL make such optimizations possible
- joins
    - joins are another sort of transformation on Pair RDDs. They’re used to combine multiple datasets 
    - inner joins(`join`)
        - inner joins return a new RDD containing combined pairs whose keys are present in both input RDDs
    - outer joins(`leftOuterJoin`/`rightOuterJoin`)
        - outer joins return a new RDD containing combined pairs whose keys don’t have to be present in both input RDDs
        - with outer joins, we can decide which RDD’s keys are most essential to keep–the left, or the right RDD in the join expression
- DataFrames
    - immutable once constructed
    - track lineage information to efficiently recompute lost data
    - enable operations on collection of elements in parallel
    - construct DataFrames by
        - parallelizing existing Python collections (lists)
        - transforming an existing Spark or pandas DFs
        - from files in HDFS or any other storage system
    - each row is a Row object, and the fields in a Row can be accessed like attributes
    - two types of operations
        - transformations: lazy
            - Spark uses Catalyst to optimize the required calculations
            - Spark recovers from failures and slow workers
            - User Defined Function Transformations: UDF takes named or lambda function and the return type of the function
        ![](https://i.imgur.com/jNGcNJc.png)
        ![](https://i.imgur.com/RStUlkX.png)
        - actions: eager
        ![](https://i.imgur.com/alebsQM.png)
- Spark program ifecycle
    - create DataFrames from external data or createDataFrame from a collection in driver program
    - lazily transform them into new DataFrames
    - `cache()` some DataFrames for reuse
    - perform actions to execute parallel computation and produce results